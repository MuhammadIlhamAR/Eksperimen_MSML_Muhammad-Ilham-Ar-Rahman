# -*- coding: utf-8 -*-
"""Copy of Template Eksperimen MSML

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fn2uL8wGll9YA80a7Rs2f75ZU1yPThv7

# **1. Perkenalan Dataset**

Sumber Dataset : https://www.kaggle.com/datasets/kurniatilaelimunifah/prsa-data-aotizhongxin

# **2. Import Library**

Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning.
"""

# Install library yang dibutuhkan
!pip install pandas numpy scikit-learn imbalanced-learn mlflow joblib matplotlib seaborn
!pip install -U imbalanced-learn
!pip install dagshub
!pip install dagshub mlflow

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
import mlflow
import mlflow.sklearn
import mlflow.keras
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
import dagshub
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve

"""# **3. Memuat Dataset**

Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.

Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.

Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

file_path = '/content/drive/MyDrive/Tugas Akhir MSML/PRSA_Data_Aotizhongxin.csv'

df = pd.read_csv(file_path)

print("Beberapa baris pertama dari dataset:")
print(df.head())

"""# **4. Exploratory Data Analysis (EDA)**

Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.

Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan.
"""

df.columns

df.shape

df.info()

df.isna().sum()

df.duplicated().sum()

# Mengatur ukuran figure
plt.figure(figsize=(30, 16))

# Loop melalui setiap kolom dalam dataset
df_select_numeric = df.select_dtypes(include=['number']) # Hanya kolom numerik
for i, col in enumerate(df_select_numeric.columns, 1):
    plt.subplot(2, (len(df_select_numeric.columns) + 1) // 2, i)
    sns.boxplot(y=df_select_numeric[col])
    plt.title(f'Boxplot of {col}')
    plt.xlabel('')

plt.tight_layout()

df_numeric = df[['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']]
df_numeric.describe()

# Set the figure size
plt.figure(figsize=(15, 10))

# Iterate through each numeric column and create a histogram
for i, col in enumerate(df_numeric.columns):
    plt.subplot(4, 3, i + 1)  # Adjust subplot layout as needed
    sns.histplot(df_numeric[col], kde=True)  # Added kde for density estimation
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

rain_avg = df.groupby(['year', 'month'])['RAIN'].mean().reset_index()

# Buat pivot table untuk heatmap
rain_pivot = rain_avg.pivot(index='year', columns='month', values='RAIN')

# Plot menggunakan heatmap
plt.figure(figsize=(24, 12))
sns.heatmap(rain_pivot, cmap="Blues", annot=True, fmt=".8f")
plt.title("Rata-rata Curah Hujan (mm) dari Tahun ke Tahun Berdasarkan Bulan")
plt.xlabel("Bulan")
plt.ylabel("Tahun")
plt.show()

# Set the figure size
plt.figure(figsize=(15, 10))

# Iterate through each numeric column and create a histogram
for i, col in enumerate(df_numeric.columns):
    plt.subplot(4, 3, i + 1)  # Adjust subplot layout as needed
    sns.scatterplot(x=df['RAIN'], y=df_numeric[col])  # Added kde for density estimation
    plt.title(f'Scatter Plot of RAIN vs {col}')
    plt.xlabel('RAIN')
    plt.ylabel(col)

plt.tight_layout()
plt.show()

df_numeric.corr()

# Group data by year and month, then sum the rainfall
rain_sum = df.groupby(['year', 'month'])['RAIN'].mean().reset_index()

# Create the line chart
plt.figure(figsize=(12, 6))
for year in rain_sum['year'].unique():
    year_data = rain_sum[rain_sum['year'] == year]
    plt.plot(year_data['month'], year_data['RAIN'], label=str(year))

plt.xlabel("Month")
plt.ylabel("Total Rainfall (mm)")
plt.title("Rata-rata curah hujan dari tahun 2013/2017")
plt.legend()
plt.grid(True)
plt.show()

"""# **5. Data Preprocessing**

Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.

Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.

Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:
1. Menghapus atau Menangani Data Kosong (Missing Values)
2. Menghapus Data Duplikat
3. Normalisasi atau Standarisasi Fitur
4. Deteksi dan Penanganan Outlier
5. Encoding Data Kategorikal
6. Binning (Pengelompokan Data)

Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur.
"""

df = df.drop(columns = ['station'])

# Mengisi missing value kolom numerik dengan rata-rata
for col in ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']:
    df[col] = df[col].fillna(value=df[col].mean())

# Mengisi missing value kolom kategorik dengan modus
df['wd'] = df['wd'].fillna(df['wd'].mode()[0])

#Melihat kembali data apakah masih ada missing value
print(df.isna().sum())

# Imputasi outlier dengan IQR
def impute_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])
    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])

    return df

df_no_outlier = df.copy()
# Lakukan hal yang sama untuk kolom lain yang mengandung outlier
for col in ['PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'WSPM', 'PM2.5', 'RAIN']:
    df_no_outlier = impute_outliers_iqr(df_no_outlier, col)

# Mengatur ukuran figure
plt.figure(figsize=(30, 16))

# Loop melalui setiap kolom dalam dataset
df_select_numeric = df_no_outlier.select_dtypes(include=['number']) # Hanya kolom numerik
for i, col in enumerate(df_select_numeric.columns, 1):
    plt.subplot(2, (len(df_select_numeric.columns) + 1) // 2, i)
    sns.boxplot(y=df_select_numeric[col])
    plt.title(f'Boxplot of {col}')
    plt.xlabel('')

plt.tight_layout()

df_no_outlier['RAIN'].value_counts()

rain = df['RAIN']
for index, value in rain.items():
  if value == 0:
    df.loc[index, 'RAIN_Category'] = 'Tidak Hujan'
  else:
    df.loc[index, 'RAIN_Category'] = 'Hujan'

df['RAIN_Category'].value_counts()

# Lakukan hal yang sama untuk kolom lain yang mengandung outlier
for col in ['PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'WSPM', 'PM2.5']:
    impute_outliers_iqr(df, col)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.over_sampling import SMOTE
import mlflow
import mlflow.sklearn
import mlflow.keras
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
import dagshub
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve

def load_data(file_path):
    df = pd.read_csv(file_path)
    print(f"Dataset dimuat dari {file_path}")
    return df

def feature_engineering(df):
    scaler = StandardScaler()

    # Identify numerical columns for scaling after other processing
    # Assuming X will have 'PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'WSPM', 'month', 'day', 'wd_encoded'
    # Let's scale the pollution and weather variables, excluding day/month for now if they are treated as cyclical or categorical later.
    cols_to_scale = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'WSPM']

    existing_cols_to_scale = [col for col in cols_to_scale if col in df.columns]

    if existing_cols_to_scale:
        df[existing_cols_to_scale] = scaler.fit_transform(df[existing_cols_to_scale])
        print("Selected numerical features successfully normalized.")
    else:
        print("No numerical features to normalize found in the DataFrame.")
    return df

def apply_smote(X, y):
    smote = SMOTE(sampling_strategy='auto', random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, y)
    print(f"Jumlah kelas setelah SMOTE:\n{pd.Series(y_resampled).value_counts()}")
    return X_resampled, y_resampled

def save_data(df, output_path):
    df.to_csv(output_path, index=False)
    print(f"Data disimpan ke {output_path}")

# Main function yang menjalankan seluruh proses
def main(input_file, output_file):
    # Load data
    df = load_data(input_file)

    # --- Data Preprocessing Steps ---
    # 1. Drop 'No' and 'station' columns as they are not needed for modeling
    df = df.drop(columns=['No', 'station'])

    # 2. Impute missing values for numerical columns with their mean
    all_numerical_cols = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']
    for col in all_numerical_cols:
        df[col] = df[col].fillna(value=df[col].mean())

    # 3. Impute missing values for categorical column 'wd' with its mode
    df['wd'] = df['wd'].fillna(df['wd'].mode()[0])

    # 4. Create 'RAIN_Category' column based on the 'RAIN' feature (after missing value imputation)
    # This is done *before* outlier imputation on RAIN to preserve class diversity.
    df['RAIN_Category'] = np.where(df['RAIN'] == 0, 'Tidak Hujan', 'Hujan')

    # 5. Define numerical columns for outlier imputation, excluding 'RAIN'
    # as its categorical version (RAIN_Category) is now the target.
    features_for_outlier_imputation = [col for col in all_numerical_cols if col != 'RAIN']

    # 6. Handle outliers using IQR method for relevant numerical features
    def impute_outliers_iqr_local(data_frame, column):
        Q1 = data_frame[column].quantile(0.25)
        Q3 = data_frame[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        data_frame[column] = np.where(data_frame[column] < lower_bound, lower_bound, data_frame[column])
        data_frame[column] = np.where(data_frame[column] > upper_bound, upper_bound, data_frame[column])
        return data_frame

    for col in features_for_outlier_imputation:
        df = impute_outliers_iqr_local(df, col)

    # 7. Encode the categorical 'wd' column
    le = LabelEncoder()
    df['wd_encoded'] = le.fit_transform(df['wd'])
    df = df.drop(columns=['wd']) # Drop the original 'wd' column

    print("Data cleaning and initial feature preparation complete.")
    # --- End of Data Preprocessing Steps ---

    # Pisahkan fitur (X) dan target (y)
    # 'RAIN' is dropped here because 'RAIN_Category' is the new target
    X = df.drop(['RAIN', 'RAIN_Category', 'year', 'hour'], axis=1)
    y = df['RAIN_Category']

    # Terapkan SMOTE untuk menangani imbalance
    X_resampled, y_resampled = apply_smote(X, y)

    # Lakukan rekayasa fitur (normalization/scaling) on the resampled features X
    # Ensure only numerical columns in X are passed to feature_engineering
    X_scaled = feature_engineering(pd.DataFrame(X_resampled, columns=X.columns))

    # Combine scaled features with resampled target
    df_processed = X_scaled.copy()
    df_processed['RAIN_Category'] = y_resampled

    # Simpan data yang sudah diproses
    save_data(df_processed, output_file)
    print("Preprocessing selesai. Data siap untuk digunakan.")

# Untuk menjalankan fungsi secara otomatis jika file dieksekusi
if __name__ == "__main__":
    input_file = "/content/drive/MyDrive/Tugas Akhir MSML/PRSA_Data_Aotizhongxin.csv"
    output_file = "PRSA_Data_Aotizhongxin_preprocessing.csv"
    main(input_file, output_file)

"""# **6. Modelling**"""

dagshub.init(repo_owner="m.ilham2408", repo_name="my-first-repo", mlflow=True)

mlflow.set_tracking_uri("https://dagshub.com/m.ilham2408/my-first-repo.mlflow")
mlflow.set_experiment("Tugas Akhir MSML")

df = pd.read_csv("/content/PRSA_Data_Aotizhongxin_preprocessing.csv")

# The 'RAIN_Category' column is already present and balanced from the preprocessing step.
# It also has already been mapped back to categorical labels ('Tidak Hujan', 'Hujan') before saving.

X = df.drop(['RAIN_Category'], axis=1) # Only drop the target column to get features
y = df['RAIN_Category']  # Target

# Convert target variable to numerical for roc_curve and other metrics
# 'Tidak Hujan' = 0, 'Hujan' = 1
y = y.map({'Tidak Hujan': 0, 'Hujan': 1})

# Add a check to explicitly show the y.value_counts() after loading to confirm the data balance
print("Value counts of RAIN_Category after loading and mapping:")
print(y.value_counts())

# Membagi data menjadi train dan test, ensuring stratification for class balance
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_score, recall_score, f1_score

model = RandomForestClassifier(n_estimators=100, random_state=42)

# Menentukan eksperimen MLflow dan memulai tracking
mlflow.set_experiment("Tugas Akhir MSML")

with mlflow.start_run(run_name="Initial RandomForest Model"):
    # Menyimpan parameter model
    mlflow.log_param("n_estimators", 100)
    mlflow.log_param("random_state", 42)

    # Melatih model
    model.fit(X_train, y_train)

    # Prediksi dan evaluasi
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    # Log metrik standar
    mlflow.log_metric("accuracy", accuracy)

    # Log metrik tambahan (precision, recall, f1-score, roc_auc_score)
    precision_0 = precision_score(y_test, y_pred, pos_label=0)
    recall_0 = recall_score(y_test, y_pred, pos_label=0)
    f1_0 = f1_score(y_test, y_pred, pos_label=0)

    precision_1 = precision_score(y_test, y_pred, pos_label=1)
    recall_1 = recall_score(y_test, y_pred, pos_label=1)
    f1_1 = f1_score(y_test, y_pred, pos_label=1)

    mlflow.log_metric("precision_class_0", precision_0)
    mlflow.log_metric("recall_class_0", recall_0)
    mlflow.log_metric("f1_score_class_0", f1_0)

    mlflow.log_metric("precision_class_1", precision_1)
    mlflow.log_metric("recall_class_1", recall_1)
    mlflow.log_metric("f1_score_class_1", f1_1)

    roc_auc = roc_auc_score(y_test, y_pred)
    mlflow.log_metric("roc_auc_score", roc_auc)

    # Buat dan simpan Confusion Matrix sebagai artefak
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
                xticklabels=['Tidak Hujan', 'Hujan'], yticklabels=['Tidak Hujan', 'Hujan'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.savefig("confusion_matrix.png")
    mlflow.log_artifact("confusion_matrix.png")
    plt.close()

    # Buat dan simpan ROC Curve sebagai artefak
    fpr, tpr, _ = roc_curve(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.tight_layout()
    plt.savefig("roc_curve.png")
    mlflow.log_artifact("roc_curve.png")
    plt.close()

    # Menyimpan model ke MLflow
    mlflow.sklearn.log_model(model, "initial_random_forest_model")

    print(f"Accuracy: {accuracy}")
    print(f"ROC AUC Score: {roc_auc}")

param_dist = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
}

# Mencari parameter terbaik
random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, random_state=42)
with mlflow.start_run(run_name="RandomizedSearchCV Best Model"):
    # Menyimpan parameter dan model (pindahkan log_params setelah fit)
    mlflow.log_param("search_method", "RandomizedSearchCV")

    random_search.fit(X_train, y_train)

    # Sekarang best_params_ sudah tersedia setelah fit()
    mlflow.log_params(random_search.best_params_)

    # Menyimpan hasil metrik dan model terbaik
    best_model = random_search.best_estimator_
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    mlflow.log_metric("accuracy", accuracy)

    # Log metrik tambahan (precision, recall, f1-score, roc_auc_score)
    precision_0 = precision_score(y_test, y_pred, pos_label=0)
    recall_0 = recall_score(y_test, y_pred, pos_label=0)
    f1_0 = f1_score(y_test, y_pred, pos_label=0)

    precision_1 = precision_score(y_test, y_pred, pos_label=1)
    recall_1 = recall_score(y_test, y_pred, pos_label=1)
    f1_1 = f1_score(y_test, y_pred, pos_label=1)

    mlflow.log_metric("precision_class_0", precision_0)
    mlflow.log_metric("recall_class_0", recall_0)
    mlflow.log_metric("f1_score_class_0", f1_0)

    mlflow.log_metric("precision_class_1", precision_1)
    mlflow.log_metric("recall_class_1", recall_1)
    mlflow.log_metric("f1_score_class_1", f1_1)

    roc_auc = roc_auc_score(y_test, y_pred)
    mlflow.log_metric("roc_auc_score", roc_auc)

    # Buat dan simpan Confusion Matrix sebagai artefak
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
                xticklabels=['Tidak Hujan', 'Hujan'], yticklabels=['Tidak Hujan', 'Hujan'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix - Best Model')
    plt.tight_layout()
    plt.savefig("confusion_matrix_best_model.png")
    mlflow.log_artifact("confusion_matrix_best_model.png")
    plt.close()

    # Buat dan simpan ROC Curve sebagai artefak
    fpr, tpr, _ = roc_curve(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic - Best Model')
    plt.legend(loc="lower right")
    plt.tight_layout()
    plt.savefig("roc_curve_best_model.png")
    mlflow.log_artifact("roc_curve_best_model.png")
    plt.close()

    mlflow.sklearn.log_model(best_model, "best_random_forest_model")

    print(f"Best Model Accuracy: {accuracy}")
    print(f"Best Model ROC AUC Score: {roc_auc}")

if y_pred.dtype == object and isinstance(y_pred[0], str):
    y_pred_numeric = pd.Series(y_pred).map({'Tidak Hujan': 0, 'Hujan': 1}).values
else:
    y_pred_numeric = y_pred

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_numeric)
print("Confusion Matrix:\n", cm)

# Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred_numeric))

# ROC Curve (untuk klasifikasi biner)
fpr, tpr, thresholds = roc_curve(y_test, y_pred_numeric)
print("ROC AUC Score: ", roc_auc_score(y_test, y_pred_numeric))

# Optionally, you can visualize the ROC curve
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label='ROC curve')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for random classifier
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

with mlflow.start_run():
    mlflow.log_param("example", "Saving model to DagsHub")
    mlflow.sklearn.log_model(model, "model")
    print("Model saved to DagsHub")
